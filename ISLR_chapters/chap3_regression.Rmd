---
title: "Chapter 3: Linear Regression"
---

```{r load_libraries}
library(MASS)
library(ISLR2)
library(stargazer)     # Summarizing
library(tidyverse)     # Data processing
library(ggthemes)      # Plotting
library(car)           # VIF
```

```{r exploration}
# medv: median house value
# rm: average number of rooms per house
# age: age of house
# lstat: percentage of household with low socioeconomic status

head(Boston)
```

`names(lm.fit)` will tell us what pieces of information is stored in lm.fit:
* **coefficients** 
* **residuals**
* **effects**
* **rank**
* **fitted.values** 
* **assign**       
* **qr**
* **df.residual** 
* **xlevels**
* **call**
* **terms** 
* **model**

```{r linear_model}
lm.fit <- lm(medv ~ lstat, data = Boston)
summary(lm.fit)
```

**Coefficients**

To extract coefficients, we can use the `coef()` function:

```{r coefficients}
coef(lm.fit)
```

**Confidence Interval**

```{r confidence_interval}
confint(lm.fit)
```

For instance, the 95 % confidence interval associated with a `lstat` value of 10 is (24.47, 25.63), and the 95 % prediction interval is (12.828, 37.28). As expected, the confidence and prediction intervals are centered around the same point (a predicted value of 25.05 for medv when `lstat` equals 10), but the latter are substantially wider.

```{r}
predict(lm.fit, 
        data.frame(lstat = (c(5, 10, 15))),
        interval = "confidence")

predict(lm.fit, 
        data.frame(lstat = (c(5, 10, 15))),
        interval = "prediction")
```

```{r}
plot(Boston$lstat, Boston$medv)
# Least-squared regression line
abline(lm.fit)
```

```{r other_plots}
# plot(Boston$lstat, Boston$medv)
# abline(lm.fit, lwd = 3)
# abline(lm.fit, lwd = 3, col = 'red')
# plot(Boston$lstat, Boston$medv, col = 'red')
# plot(Boston$lstat, Boston$medv, pch = 20)
plot(Boston$lstat, Boston$medv, pch = "+")
# plot(1:20, 1:20, pch = 1:20)
```

`par(mfrow = c(2, 2))` divides the plotting region into a 2 × 2 grid of panels.

```{r}
par(mfrow = c(2, 2))
plot(lm.fit)
```

**Residuals and Outliers**

Another important assumption of the linear regression model is that the error terms have a constant variance. The standard errors, confidence intervals, and hypothesis tests associated with the linear model rely upon this assumption. Plotting the residuals against the predictions is a way to confirm this assumption.

* `residuals()`
* `rstudent()`: will return the studentized residuals. A studentized residual is the quotient resulting from the division of a residual by an estimate of its standard deviation. It is a form of a Student's t-statistic, with the estimate of error varying between points.

```{r}
plot(predict(lm.fit), residuals(lm.fit))
```

```{r}
tibble(predictions = predict(lm.fit), residuals = residuals(lm.fit)) %>% 
  ggplot(aes(x = predictions, y = residuals)) +
  geom_point(col = 'blue') +
  theme_hc()
```

* Outliers are observations for which the response $y_i$ is unusual given the predictor $x_i$.
* Removing the outlier has little effect on the least squares line: it leads to almost no change in the slope, and a miniscule reduction in the intercept. It is typical for an outlier that does not have an unusual predictor value to have little effect on the least squares fit.
* Inclusion of the outliers may decrease goodness of fit (R-squared) and increase RSE.
* It can be difficult to decide how large a residual needs to be before we consider the point to be an outlier. To address this problem, instead of plotting the residuals, we can plot the studentized residuals, computed by dividing each residual by its estimated standard error. **Observations whose studentized residuals are greater than 3 in abso- lute value are possible outliers.**
* If we believe that an outlier has occurred due to an error in data collection or recording, then one solution is to simply remove the observation. However, care should be taken, since an outlier may instead indicate a deficiency with the model, such as a missing predictor.

```{r}
plot(predict(lm.fit), rstudent(lm.fit))
```

**Leverage statistics**

*  Observations with high leverage high have an unusual value for $x_i$ (the predictor value for these observations are large relative to the other observations).
* We observe that removing the high leverage observation has a much more substantial impact on the least squares line than removing the outlier.
* This problem is more pronounced in **multiple regression** settings with more than two predictors, because then there is no simple way to plot all dimensions of the data simultaneously.
* In order to quantify an observation’s leverage, we compute the leverage statisti: $h_i = \frac{1}{n} + \frac{(x_i - \overline{x})^2}{\sum_{i'=1}^{n} (x'_i - \overline{x})^2}$
* The leverage statistic hi is always between $\frac{1}{n}$ and 1, and the average leverage for all the observations is always equal to $\frac{p + 1}{n}$. So if a given observation has a leverage statistic that greatly exceeds $\frac{p + 1}{n}$, then we may suspect that the corresponding point has high leverage.
* On the basis of the residual plots, there is some evidence of non-linearity. Leverage statistics can be computed for any number of predictors using the **`hatvalues()`** function. The `which.max()` function identifies the index of the largest element of a vector. In this case, it tells us which observation has the largest leverage statistic.

```{r}
plot(hatvalues(lm.fit))
```

```{r}
which.max(hatvalues(lm.fit))
```

**Multiple Linear Regression**

```{r}
lm.fit <- lm(medv ~ lstat + age, data = Boston)
summary(lm.fit)
```

```{r}
# Use all predictors
lm.fit <- lm(medv ~ ., data = Boston)
summary(lm.fit)
```

```{r}
lm.fit1 <- lm(medv ~ . - age, data = Boston)
summary(lm.fit1)
```

```{r}
lm.fit1 <- update(lm.fit, ~ . - age)
```

**Getting components of `lm.fit`**

```{r}
# Use ?summary.lm to see what is available

# R-squared
print(summary(lm.fit)$r.sq)

# RSE
print(summary(lm.fit)$sigma)
```

**Variance inflation factors**

* A simple way to detect collinearity is to look at the correlation matrix of the predictors.
* Unfortunately, not all collinearity problems can be detected by inspection of the correlation matrix: it is possible for collinear- ity to exist between three or more variables even if no pair of variables has a particularly high correlation. We call this situation multicollinearity.
* Instead of inspecting the correlation matrix, a better way to assess multi- collinearity is to compute the variance inflation factor (VIF).
* The VIF is the ratio of the variance of $\beta_j$ when fitting the full model divided by the variance of $\beta_j$ if fit on its own.
* The smallest possible value for VIF is 1, which indicates the complete absence of collinearity. Typically in practice there is a small amount of collinearity among the predictors.
* As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity. 

```{r}
vif(lm.fit)
```

**Interaction Term**

* The syntax `lstat:black` tells R to include an interaction term between lstat and black. The syntax `lstat * age` simultaneously includes lstat, age, and the interaction term `lstat × age` as predictors; it is a short-hand for `lstat + age + lstat:age`.

```{r}
summary(lm(medv ~ lstat * age, data = Boston))
```

**Non-linear Transformations of the Predictors and ANOVA**

```{r}
# The function I() is needed since the ^ has a special meaning in a formula object.
lm.fit2 <- lm(medv ~ lstat + I(lstat^2), data = Boston)
summary(lm.fit2)
```

* We use the `anova()` function to further quantify the extent to which the quadratic fit is superior to the linear fit.
* The `anova()` function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior.
* Here the F-statistic is 135 and the associated p-value is virtually zero. This provides very clear evidence that the model containing the predictors $lstat$ and $lstat^2$ is far superior to the model that only contains the predictor $lstat$. 
* We see that when the $lstat^2$ term is included in the model, there is little discernible pattern in the residuals.

```{r}
lm.fit <- lm(medv ~ lstat, data = Boston)
anova(lm.fit, lm.fit2)
```

```{r}
par(mfrow = c(2, 2))
plot(lm.fit2)
```

```{r}
# Higher polynomials
lm.fit5 <- lm(medv ~ poly(lstat, 5), data = Boston)
summary(lm.fit5)
```

* By default, the `poly()` function orthogonalizes the predictors: this means that the features output by this function are not simply a sequence of powers of the argument. However, a linear model applied to the output of the poly() function will have the same fitted values as a linear model applied to the raw polynomials (although the coefficient estimates, standard errors, and p-values will differ).

```{r}
# Higher polynomials
lm.fit5 <- lm(medv ~ poly(lstat, 5, raw=TRUE), data = Boston)
summary(lm.fit5)
```

```{r}
summary(lm(medv ~ log(rm), data = Boston))
```

**Qualitative Predictors**

* `lm()`: given a qualitative variable such as `Shelveloc`, R generates dummy variables automatically.

```{r}
head(Carseats)
```

```{r}
lm.fit <- lm(Sales ~ . + Income:Advertising + Price:Age, data = Carseats)
summary(lm.fit)
```

* The `contrasts()` function returns the coding that R uses for the dummy variables.
* The fact that the coefficient for `ShelveLocGood` in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And `ShelveLocMedium` has a smaller positive coefficient, indicating that a medium shelving location is associated with higher sales than a bad shelving location but lower sales than a good shelving location.

```{r}
attach(Carseats)
contrasts(ShelveLoc)
```

